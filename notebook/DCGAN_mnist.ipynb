{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "mnist = input_data.read_data_sets('MNIST_data/', reshape=False, validation_size=0)\n",
    "train_x = mnist.train.images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_epoch = 20\n",
    "batch_size = 100\n",
    "learning_rate = 0.0002\n",
    "kernel_initializer = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def leakyrelu(x, a=0.2):\n",
    "  return tf.maximum(a*x, x)\n",
    "\n",
    "relu = tf.nn.relu\n",
    "\n",
    "def random_noise(batch_size):\n",
    "  return np.random.normal(0, 1, size=[batch_size, 1, 1, 100])\n",
    "\n",
    "def generator(x, isTrain=True, reuse=False):\n",
    "  with tf.variable_scope(name_or_scope='generator', reuse=reuse):\n",
    "\n",
    "    conv1 = tf.layers.conv2d_transpose(x,filters=1024,kernel_size=4, strides=(1,1),padding='valid',kernel_initializer=kernel_initializer)\n",
    "    relu1 = relu(tf.layers.batch_normalization(conv1,training=isTrain))\n",
    "\n",
    "    conv2 = tf.layers.conv2d_transpose(relu1,filters=512,kernel_size=4, strides=(2,2),padding='same',kernel_initializer=kernel_initializer)\n",
    "    relu2 = relu(tf.layers.batch_normalization(conv2,training=isTrain))\n",
    "\n",
    "    conv3 = tf.layers.conv2d_transpose(relu2,filters=256,kernel_size=4, strides=(2,2),padding='same',kernel_initializer=kernel_initializer)\n",
    "    relu3 = relu(tf.layers.batch_normalization(conv3,training=isTrain))\n",
    "\n",
    "    conv4 = tf.layers.conv2d_transpose(relu3,filters=128,kernel_size=4, strides=(2,2),padding='same',kernel_initializer=kernel_initializer)\n",
    "    relu4 = relu(tf.layers.batch_normalization(conv4,training=isTrain))\n",
    "\n",
    "    conv5 = tf.layers.conv2d_transpose(relu4,filters=1,kernel_size=4, strides=(2,2),padding='same',kernel_initializer=kernel_initializer)\n",
    "    output = tf.nn.tanh(conv5)\n",
    "\n",
    "    return output\n",
    "\n",
    "def discriminator(x, isTrain=True, reuse=False):\n",
    "  with tf.variable_scope('discriminator', reuse=reuse):\n",
    "\n",
    "    conv1 = tf.layers.conv2d(x,filters=128,kernel_size=4, strides=(2,2),padding='same',kernel_initializer=kernel_initializer)\n",
    "    leakyrelu1 = leakyrelu(conv1)\n",
    "\n",
    "    conv2 = tf.layers.conv2d(leakyrelu1,filters=256,kernel_size=4, strides=(2,2),padding='same',kernel_initializer=kernel_initializer)\n",
    "    leakyrelu2 = leakyrelu(tf.layers.batch_normalization(conv2,training=isTrain))\n",
    "\n",
    "    conv3 = tf.layers.conv2d(leakyrelu2,filters=512,kernel_size=4, strides=(2,2),padding='same',kernel_initializer=kernel_initializer)\n",
    "    leakyrelu3 = leakyrelu(tf.layers.batch_normalization(conv3,training=isTrain))\n",
    "\n",
    "    conv4 = tf.layers.conv2d(leakyrelu3,filters=1024,kernel_size=4, strides=(2,2),padding='same',kernel_initializer=kernel_initializer)\n",
    "    leakyrelu4 = leakyrelu(tf.layers.batch_normalization(conv4,training=isTrain))\n",
    "\n",
    "    conv5 = tf.layers.conv2d(leakyrelu4,filters=1,kernel_size=4, strides=(1,1),padding='valid',kernel_initializer=kernel_initializer)\n",
    "    \n",
    "    return conv5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_time0 = time.time()\n",
    "g = tf.Graph()\n",
    "\n",
    "with g.as_default():\n",
    "\n",
    "  x = tf.placeholder(tf.float32, shape=(None,64,64,1))\n",
    "  z = tf.placeholder(tf.float32, shape=(None,1,1,100))\n",
    "  isTrain = tf.placeholder(dtype=tf.bool)\n",
    "\n",
    "  G_z = generator(z)\n",
    "  fake = discriminator(G_z, isTrain)\n",
    "  real = discriminator(x, isTrain, reuse=True)\n",
    "\n",
    "  D_loss_real = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=real, labels=tf.ones([batch_size,1,1,1])))\n",
    "  D_loss_fake = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=fake, labels=tf.zeros([batch_size,1,1,1])))  \n",
    "  D_loss = D_loss_real + D_loss_fake\n",
    "  G_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=fake, labels=tf.ones([batch_size, 1, 1, 1])))\n",
    "\n",
    "  T_vars = tf.trainable_variables() # return list\n",
    "  D_vars = [var for var in T_vars if var.name.startswith('discriminator')]\n",
    "  G_vars = [var for var in T_vars if var.name.startswith('generator')]\n",
    "\n",
    "  optimizer = tf.train.AdamOptimizer(learning_rate,beta1=.5)\n",
    "  D_optim = optimizer.minimize(D_loss, var_list=D_vars)\n",
    "  G_optim = optimizer.minimize(G_loss, var_list=G_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session(graph=g) as sess:\n",
    "  sess.run(tf.global_variables_initializer())\n",
    "  total_batches = len(train_x) // batch_size\n",
    "  sample_noise = random_noise(10)\n",
    "\n",
    "  train_x = tf.image.resize_images(train_x, [64, 64]).eval()\n",
    "  train_x = 2 * train_x - 1\n",
    "\n",
    "  for epoch in range(total_epoch):\n",
    "    epoch_time0 = time.time()\n",
    "    \n",
    "    for batch in range(total_batches):\n",
    "      t0 = time.time()\n",
    "\n",
    "      batch_x = train_x[batch*batch_size:(batch+1)*batch_size]\n",
    "      noise = random_noise(batch_size)\n",
    "      sess.run(D_optim, feed_dict={x : batch_x, z : noise, isTrain : True})\n",
    "      noise = random_noise(batch_size)\n",
    "      sess.run(G_optim, feed_dict={z : noise, isTrain : True})\n",
    "      gl, dl = sess.run([G_loss, D_loss], feed_dict={x : batch_x, z : noise, isTrain : False})\n",
    "\n",
    "      t1 = time.time()\n",
    "      print('epoch : {:02d}/{:02d}'.format(epoch+1, total_epoch), end=' | ')\n",
    "      print('batch : {:03d}/{:03d}'.format(batch+1, total_batches), end=' | ')\n",
    "      print('time spent : {:.3f} sec'.format(t1-t0), end=' | ')\n",
    "      print('gl : {:.6f} | dl : {:.6f}'.format(gl, dl))\n",
    "\n",
    "      epoch_time1 = time.time()\n",
    "      epoch_time = epoch_time1 - epoch_time0\n",
    "      print('==================== EPOCH :', epoch+1, ' ended ====================')\n",
    "      print('Generator Loss : {:.6f}\\nDiscriminator Loss : {:.6f}'.format(gl, dl))\n",
    "      print('Time Spent in This Epoch : {:.3f}'.format(epoch_time))\n",
    "\n",
    "      generated = sess.run(G_z, feed_dict={z : sample_noise, isTrain : False}) / 2 + .5\n",
    "      # generated = (generated + 1) * 255 / 2\n",
    "      fig, ax = plt.subplots(1, 10, figsize=(10, 1))\n",
    "      for i in range(10):\n",
    "        ax[i].set_axis_off()\n",
    "        ax[i].imshow(generated[i,:,:,0])\n",
    "      plt.savefig('/gdrive/My Drive/Simple_DCGAN/{}.png'.format(str(epoch+1).zfill(3)), bbox_inches='tight')\n",
    "      plt.close(fig)\n",
    "\n",
    "  global_time1 = time.time()\n",
    "  global_time_spent = global_time1 - global_time0\n",
    "  print(('=' * 40 + '\\n') * 3, 'Optimization Has Been Completed!!')\n",
    "  print('Total Time Spent : {:.3f}'.format(global_time_spent))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
