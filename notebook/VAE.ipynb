{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"VAE.ipynb","provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"XfzdkgkofeLL","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":124},"outputId":"fdb38847-f64b-4a8e-f010-91d30a0c7e9c","executionInfo":{"status":"ok","timestamp":1575003018449,"user_tz":-540,"elapsed":28853,"user":{"displayName":"JaeHeon Kwon","photoUrl":"","userId":"08814146519047845301"}}},"source":["from google.colab import drive\n","drive.mount('/gdrive', force_remount=True)"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /gdrive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ClwOORxpfjif","colab_type":"code","colab":{}},"source":["from __future__ import print_function\n","import argparse\n","import torch\n","import torch.utils.data\n","from torch import nn, optim\n","from torch.nn import functional as F\n","from torchvision import datasets, transforms\n","from torchvision.utils import save_image"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"sNLdi36zfu4Z","colab_type":"code","colab":{}},"source":["batch_size = 100\n","trainset = datasets.MNIST(root='./mnist_data/', train=True,\n","                               transform=transforms.ToTensor(), download=True)\n","trainloader = torch.utils.data.DataLoader(trainset,batch_size=batch_size,\n","                                          shuffle=True, drop_last=True)\n","testset = datasets.MNIST(root='./mnist_data/', train=False,\n","                               transform=transforms.ToTensor(), download=True)\n","testloader = torch.utils.data.DataLoader(testset,batch_size=batch_size,\n","                                          shuffle=True, drop_last=True)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"_RRTO86kgq_R","colab_type":"code","colab":{}},"source":["class VAE(nn.Module):\n","  def __init__(self):\n","    super(VAE, self).__init__()\n","    self.fc1 = nn.Linear(784,512)\n","    self.fc2 = nn.Linear(512,256)\n","    self.fc31 = nn.Linear(256,2)\n","    self.fc32 = nn.Linear(256,2)\n","    self.fc4 = nn.Linear(2,256)\n","    self.fc5 = nn.Linear(256,512)\n","    self.fc6 = nn.Linear(512,784)\n","\n","  def encode(self, x):\n","    h1 = F.relu(self.fc1(x))\n","    h1 = F.relu(self.fc2(h1))\n","    return self.fc31(h1), self.fc32(h1) #mu, logvar\n","\n","  def reparameterize(self, mu, logvar):\n","    std = torch.exp(0.5*logvar)\n","    eps = torch.rand_like(std)\n","    return mu + eps * std\n","\n","  def decode(self, z):\n","    h2 = F.relu(self.fc4(z))\n","    h2 = F.relu(self.fc5(h2))\n","    return torch.sigmoid(self.fc6(h2))\n","\n","  def forward(self, x):\n","    mu, logvar = self.encode(x.view(-1,784)) # sampling\n","    z = self.reparameterize(mu,logvar) # reparametrize trick을 이용하여 미분가능하게 변경\n","    return self.decode(z), mu, logvar"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"3FjclN1DiXvM","colab_type":"code","colab":{}},"source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model = VAE().to(device)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"TDMNdIeNmbt0","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":174},"outputId":"37456c29-73e9-4fad-87ee-ebbdf32dbfca","executionInfo":{"status":"ok","timestamp":1575008353471,"user_tz":-540,"elapsed":1260,"user":{"displayName":"JaeHeon Kwon","photoUrl":"","userId":"08814146519047845301"}}},"source":["model"],"execution_count":60,"outputs":[{"output_type":"execute_result","data":{"text/plain":["VAE(\n","  (fc1): Linear(in_features=784, out_features=512, bias=True)\n","  (fc2): Linear(in_features=512, out_features=256, bias=True)\n","  (fc31): Linear(in_features=256, out_features=2, bias=True)\n","  (fc32): Linear(in_features=256, out_features=2, bias=True)\n","  (fc4): Linear(in_features=2, out_features=256, bias=True)\n","  (fc5): Linear(in_features=256, out_features=512, bias=True)\n","  (fc6): Linear(in_features=512, out_features=784, bias=True)\n",")"]},"metadata":{"tags":[]},"execution_count":60}]},{"cell_type":"code","metadata":{"id":"EC9OhR3MmzR8","colab_type":"code","colab":{}},"source":["optimizer = optim.Adam(model.parameters())\n","\n","def lossfunction(recon_x, x, mu, logvar):\n","  BCE = F.binary_cross_entropy(recon_x, x.view(-1,784), reduction='sum')\n","  # 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n","  KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n","  return BCE + KLD"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"yFpMZTdMvZxa","colab_type":"code","colab":{}},"source":["def train(epoch):\n","  model.train()\n","  train_loss = 0\n","  for batch_idx, (data, _) in enumerate(trainloader):\n","    data = data.to(device)\n","    optimizer.zero_grad()\n","\n","    recon_batch, mu, logvar = model(data)\n","    loss = lossfunction(recon_batch, data ,mu, logvar)\n","\n","    loss.backward()\n","    train_loss += loss.item()\n","    optimizer.step()\n","\n","    if batch_idx % 100 == 0:\n","      print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n","          epoch, batch_idx * len(data), len(trainloader.dataset),\n","          100. * batch_idx / len(trainloader), loss.item() / len(data)))\n","  print('====> Epoch: {} Average loss: {:.4f}'.format(epoch, train_loss / len(train_loader.dataset)))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"tlimslG_xLFk","colab_type":"code","colab":{}},"source":["def train(epoch):\n","    model.train()\n","    train_loss = 0\n","    for batch_idx, (data, _) in enumerate(trainloader):\n","        data = data.to(device)\n","        optimizer.zero_grad()\n","        \n","        recon_batch, mu, logvar = model(data)\n","        loss = lossfunction(recon_batch, data, mu, logvar)\n","        \n","        loss.backward()\n","        train_loss += loss.item()\n","        optimizer.step()\n","        \n","        if batch_idx % 100 == 0:\n","            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n","                epoch, batch_idx * len(data), len(trainloader.dataset),\n","                100. * batch_idx / len(trainloader), loss.item() / len(data)))\n","    print('====> Epoch: {} Average loss: {:.4f}'.format(epoch, train_loss / len(trainloader.dataset)))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZMO_nUsdxOUR","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"25dec945-2ea8-42ab-b059-b0e8cdb77470","executionInfo":{"status":"ok","timestamp":1575009090224,"user_tz":-540,"elapsed":642629,"user":{"displayName":"JaeHeon Kwon","photoUrl":"","userId":"08814146519047845301"}}},"source":["for epoch in range(1, 51):\n","    train(epoch)\n","    test()\n","    with torch.no_grad():\n","            sample = torch.randn(64, 2).to(device)\n","            sample = model.decode(sample).cpu()\n","            save_image(sample.view(64, 1, 28, 28),\n","                       '/gdrive/My Drive/' + str(epoch) + '.png')"],"execution_count":70,"outputs":[{"output_type":"stream","text":["Train Epoch: 1 [0/60000 (0%)]\tLoss: 159.859365\n","Train Epoch: 1 [10000/60000 (17%)]\tLoss: 159.353828\n","Train Epoch: 1 [20000/60000 (33%)]\tLoss: 157.094590\n","Train Epoch: 1 [30000/60000 (50%)]\tLoss: 151.715234\n","Train Epoch: 1 [40000/60000 (67%)]\tLoss: 159.360439\n","Train Epoch: 1 [50000/60000 (83%)]\tLoss: 160.751758\n","====> Epoch: 1 Average loss: 154.5627\n","====> Test set loss: 151.7051\n","Train Epoch: 2 [0/60000 (0%)]\tLoss: 152.091572\n","Train Epoch: 2 [10000/60000 (17%)]\tLoss: 153.983574\n","Train Epoch: 2 [20000/60000 (33%)]\tLoss: 156.447803\n","Train Epoch: 2 [30000/60000 (50%)]\tLoss: 147.338545\n","Train Epoch: 2 [40000/60000 (67%)]\tLoss: 153.793730\n","Train Epoch: 2 [50000/60000 (83%)]\tLoss: 148.678711\n","====> Epoch: 2 Average loss: 149.9144\n","====> Test set loss: 149.1920\n","Train Epoch: 3 [0/60000 (0%)]\tLoss: 146.533281\n","Train Epoch: 3 [10000/60000 (17%)]\tLoss: 137.705342\n","Train Epoch: 3 [20000/60000 (33%)]\tLoss: 146.975723\n","Train Epoch: 3 [30000/60000 (50%)]\tLoss: 149.864473\n","Train Epoch: 3 [40000/60000 (67%)]\tLoss: 142.626221\n","Train Epoch: 3 [50000/60000 (83%)]\tLoss: 143.437480\n","====> Epoch: 3 Average loss: 147.3747\n","====> Test set loss: 146.8573\n","Train Epoch: 4 [0/60000 (0%)]\tLoss: 141.352061\n","Train Epoch: 4 [10000/60000 (17%)]\tLoss: 160.128867\n","Train Epoch: 4 [20000/60000 (33%)]\tLoss: 132.377764\n","Train Epoch: 4 [30000/60000 (50%)]\tLoss: 146.169854\n","Train Epoch: 4 [40000/60000 (67%)]\tLoss: 142.920430\n","Train Epoch: 4 [50000/60000 (83%)]\tLoss: 147.560723\n","====> Epoch: 4 Average loss: 145.6156\n","====> Test set loss: 145.8085\n","Train Epoch: 5 [0/60000 (0%)]\tLoss: 150.803838\n","Train Epoch: 5 [10000/60000 (17%)]\tLoss: 148.310283\n","Train Epoch: 5 [20000/60000 (33%)]\tLoss: 143.554141\n","Train Epoch: 5 [30000/60000 (50%)]\tLoss: 148.012119\n","Train Epoch: 5 [40000/60000 (67%)]\tLoss: 141.950781\n","Train Epoch: 5 [50000/60000 (83%)]\tLoss: 145.336377\n","====> Epoch: 5 Average loss: 144.6113\n","====> Test set loss: 144.6146\n","Train Epoch: 6 [0/60000 (0%)]\tLoss: 144.819961\n","Train Epoch: 6 [10000/60000 (17%)]\tLoss: 145.451924\n","Train Epoch: 6 [20000/60000 (33%)]\tLoss: 135.062646\n","Train Epoch: 6 [30000/60000 (50%)]\tLoss: 150.986699\n","Train Epoch: 6 [40000/60000 (67%)]\tLoss: 140.108232\n","Train Epoch: 6 [50000/60000 (83%)]\tLoss: 144.265996\n","====> Epoch: 6 Average loss: 143.5629\n","====> Test set loss: 143.4610\n","Train Epoch: 7 [0/60000 (0%)]\tLoss: 146.707539\n","Train Epoch: 7 [10000/60000 (17%)]\tLoss: 141.855557\n","Train Epoch: 7 [20000/60000 (33%)]\tLoss: 138.168418\n","Train Epoch: 7 [30000/60000 (50%)]\tLoss: 144.055889\n","Train Epoch: 7 [40000/60000 (67%)]\tLoss: 148.706514\n","Train Epoch: 7 [50000/60000 (83%)]\tLoss: 145.029346\n","====> Epoch: 7 Average loss: 142.5945\n","====> Test set loss: 142.9936\n","Train Epoch: 8 [0/60000 (0%)]\tLoss: 148.366406\n","Train Epoch: 8 [10000/60000 (17%)]\tLoss: 145.033320\n","Train Epoch: 8 [20000/60000 (33%)]\tLoss: 132.438398\n","Train Epoch: 8 [30000/60000 (50%)]\tLoss: 135.329443\n","Train Epoch: 8 [40000/60000 (67%)]\tLoss: 138.485312\n","Train Epoch: 8 [50000/60000 (83%)]\tLoss: 140.185039\n","====> Epoch: 8 Average loss: 141.9011\n","====> Test set loss: 142.5894\n","Train Epoch: 9 [0/60000 (0%)]\tLoss: 144.860010\n","Train Epoch: 9 [10000/60000 (17%)]\tLoss: 148.918975\n","Train Epoch: 9 [20000/60000 (33%)]\tLoss: 140.032949\n","Train Epoch: 9 [30000/60000 (50%)]\tLoss: 141.536299\n","Train Epoch: 9 [40000/60000 (67%)]\tLoss: 136.301084\n","Train Epoch: 9 [50000/60000 (83%)]\tLoss: 138.710898\n","====> Epoch: 9 Average loss: 141.3548\n","====> Test set loss: 141.0897\n","Train Epoch: 10 [0/60000 (0%)]\tLoss: 141.072314\n","Train Epoch: 10 [10000/60000 (17%)]\tLoss: 138.585225\n","Train Epoch: 10 [20000/60000 (33%)]\tLoss: 139.001553\n","Train Epoch: 10 [30000/60000 (50%)]\tLoss: 138.661123\n","Train Epoch: 10 [40000/60000 (67%)]\tLoss: 142.063232\n","Train Epoch: 10 [50000/60000 (83%)]\tLoss: 142.596758\n","====> Epoch: 10 Average loss: 140.7149\n","====> Test set loss: 140.5243\n","Train Epoch: 11 [0/60000 (0%)]\tLoss: 139.643926\n","Train Epoch: 11 [10000/60000 (17%)]\tLoss: 136.239512\n","Train Epoch: 11 [20000/60000 (33%)]\tLoss: 142.426230\n","Train Epoch: 11 [30000/60000 (50%)]\tLoss: 143.990488\n","Train Epoch: 11 [40000/60000 (67%)]\tLoss: 137.422930\n","Train Epoch: 11 [50000/60000 (83%)]\tLoss: 139.074766\n","====> Epoch: 11 Average loss: 140.0737\n","====> Test set loss: 140.9051\n","Train Epoch: 12 [0/60000 (0%)]\tLoss: 133.430684\n","Train Epoch: 12 [10000/60000 (17%)]\tLoss: 142.020957\n","Train Epoch: 12 [20000/60000 (33%)]\tLoss: 139.866650\n","Train Epoch: 12 [30000/60000 (50%)]\tLoss: 143.468877\n","Train Epoch: 12 [40000/60000 (67%)]\tLoss: 146.242900\n","Train Epoch: 12 [50000/60000 (83%)]\tLoss: 136.280537\n","====> Epoch: 12 Average loss: 139.9587\n","====> Test set loss: 140.2314\n","Train Epoch: 13 [0/60000 (0%)]\tLoss: 134.346934\n","Train Epoch: 13 [10000/60000 (17%)]\tLoss: 139.606348\n","Train Epoch: 13 [20000/60000 (33%)]\tLoss: 141.271318\n","Train Epoch: 13 [30000/60000 (50%)]\tLoss: 141.691074\n","Train Epoch: 13 [40000/60000 (67%)]\tLoss: 134.156494\n","Train Epoch: 13 [50000/60000 (83%)]\tLoss: 141.149277\n","====> Epoch: 13 Average loss: 139.3238\n","====> Test set loss: 139.4722\n","Train Epoch: 14 [0/60000 (0%)]\tLoss: 143.341709\n","Train Epoch: 14 [10000/60000 (17%)]\tLoss: 146.566689\n","Train Epoch: 14 [20000/60000 (33%)]\tLoss: 145.214453\n","Train Epoch: 14 [30000/60000 (50%)]\tLoss: 138.368604\n","Train Epoch: 14 [40000/60000 (67%)]\tLoss: 139.035586\n","Train Epoch: 14 [50000/60000 (83%)]\tLoss: 138.885342\n","====> Epoch: 14 Average loss: 139.0133\n","====> Test set loss: 139.6985\n","Train Epoch: 15 [0/60000 (0%)]\tLoss: 133.362949\n","Train Epoch: 15 [10000/60000 (17%)]\tLoss: 131.839141\n","Train Epoch: 15 [20000/60000 (33%)]\tLoss: 132.270107\n","Train Epoch: 15 [30000/60000 (50%)]\tLoss: 139.814580\n","Train Epoch: 15 [40000/60000 (67%)]\tLoss: 138.830059\n","Train Epoch: 15 [50000/60000 (83%)]\tLoss: 146.278223\n","====> Epoch: 15 Average loss: 138.9712\n","====> Test set loss: 140.0479\n","Train Epoch: 16 [0/60000 (0%)]\tLoss: 139.651992\n","Train Epoch: 16 [10000/60000 (17%)]\tLoss: 128.894453\n","Train Epoch: 16 [20000/60000 (33%)]\tLoss: 135.131133\n","Train Epoch: 16 [30000/60000 (50%)]\tLoss: 141.427949\n","Train Epoch: 16 [40000/60000 (67%)]\tLoss: 137.338682\n","Train Epoch: 16 [50000/60000 (83%)]\tLoss: 140.114688\n","====> Epoch: 16 Average loss: 138.5146\n","====> Test set loss: 138.8628\n","Train Epoch: 17 [0/60000 (0%)]\tLoss: 138.535830\n","Train Epoch: 17 [10000/60000 (17%)]\tLoss: 135.309287\n","Train Epoch: 17 [20000/60000 (33%)]\tLoss: 140.780742\n","Train Epoch: 17 [30000/60000 (50%)]\tLoss: 139.297305\n","Train Epoch: 17 [40000/60000 (67%)]\tLoss: 147.228262\n","Train Epoch: 17 [50000/60000 (83%)]\tLoss: 129.645781\n","====> Epoch: 17 Average loss: 138.3414\n","====> Test set loss: 138.8975\n","Train Epoch: 18 [0/60000 (0%)]\tLoss: 143.690996\n","Train Epoch: 18 [10000/60000 (17%)]\tLoss: 132.609570\n","Train Epoch: 18 [20000/60000 (33%)]\tLoss: 134.286846\n","Train Epoch: 18 [30000/60000 (50%)]\tLoss: 141.208369\n","Train Epoch: 18 [40000/60000 (67%)]\tLoss: 136.845059\n","Train Epoch: 18 [50000/60000 (83%)]\tLoss: 136.031758\n","====> Epoch: 18 Average loss: 137.8864\n","====> Test set loss: 139.1316\n","Train Epoch: 19 [0/60000 (0%)]\tLoss: 136.282412\n","Train Epoch: 19 [10000/60000 (17%)]\tLoss: 132.716641\n","Train Epoch: 19 [20000/60000 (33%)]\tLoss: 138.566182\n","Train Epoch: 19 [30000/60000 (50%)]\tLoss: 139.343145\n","Train Epoch: 19 [40000/60000 (67%)]\tLoss: 140.187295\n","Train Epoch: 19 [50000/60000 (83%)]\tLoss: 136.338594\n","====> Epoch: 19 Average loss: 137.6035\n","====> Test set loss: 138.5016\n","Train Epoch: 20 [0/60000 (0%)]\tLoss: 135.749268\n","Train Epoch: 20 [10000/60000 (17%)]\tLoss: 140.762285\n","Train Epoch: 20 [20000/60000 (33%)]\tLoss: 130.725781\n","Train Epoch: 20 [30000/60000 (50%)]\tLoss: 144.504502\n","Train Epoch: 20 [40000/60000 (67%)]\tLoss: 139.495283\n","Train Epoch: 20 [50000/60000 (83%)]\tLoss: 139.542988\n","====> Epoch: 20 Average loss: 137.7487\n","====> Test set loss: 139.4555\n","Train Epoch: 21 [0/60000 (0%)]\tLoss: 143.497764\n","Train Epoch: 21 [10000/60000 (17%)]\tLoss: 132.849795\n","Train Epoch: 21 [20000/60000 (33%)]\tLoss: 128.424609\n","Train Epoch: 21 [30000/60000 (50%)]\tLoss: 130.849160\n","Train Epoch: 21 [40000/60000 (67%)]\tLoss: 136.672686\n","Train Epoch: 21 [50000/60000 (83%)]\tLoss: 134.743125\n","====> Epoch: 21 Average loss: 137.8312\n","====> Test set loss: 138.3868\n","Train Epoch: 22 [0/60000 (0%)]\tLoss: 145.678652\n","Train Epoch: 22 [10000/60000 (17%)]\tLoss: 136.944121\n","Train Epoch: 22 [20000/60000 (33%)]\tLoss: 147.504873\n","Train Epoch: 22 [30000/60000 (50%)]\tLoss: 127.501592\n","Train Epoch: 22 [40000/60000 (67%)]\tLoss: 142.050020\n","Train Epoch: 22 [50000/60000 (83%)]\tLoss: 142.017988\n","====> Epoch: 22 Average loss: 137.7305\n","====> Test set loss: 138.7689\n","Train Epoch: 23 [0/60000 (0%)]\tLoss: 144.247441\n","Train Epoch: 23 [10000/60000 (17%)]\tLoss: 135.740264\n","Train Epoch: 23 [20000/60000 (33%)]\tLoss: 133.369365\n","Train Epoch: 23 [30000/60000 (50%)]\tLoss: 145.436631\n","Train Epoch: 23 [40000/60000 (67%)]\tLoss: 145.483613\n","Train Epoch: 23 [50000/60000 (83%)]\tLoss: 130.907490\n","====> Epoch: 23 Average loss: 137.5387\n","====> Test set loss: 138.7911\n","Train Epoch: 24 [0/60000 (0%)]\tLoss: 146.838867\n","Train Epoch: 24 [10000/60000 (17%)]\tLoss: 131.072568\n","Train Epoch: 24 [20000/60000 (33%)]\tLoss: 135.095527\n","Train Epoch: 24 [30000/60000 (50%)]\tLoss: 127.620381\n","Train Epoch: 24 [40000/60000 (67%)]\tLoss: 145.283193\n","Train Epoch: 24 [50000/60000 (83%)]\tLoss: 136.740098\n","====> Epoch: 24 Average loss: 136.9526\n","====> Test set loss: 137.6990\n","Train Epoch: 25 [0/60000 (0%)]\tLoss: 133.299102\n","Train Epoch: 25 [10000/60000 (17%)]\tLoss: 133.152803\n","Train Epoch: 25 [20000/60000 (33%)]\tLoss: 133.156377\n","Train Epoch: 25 [30000/60000 (50%)]\tLoss: 140.191201\n","Train Epoch: 25 [40000/60000 (67%)]\tLoss: 125.826934\n","Train Epoch: 25 [50000/60000 (83%)]\tLoss: 132.431465\n","====> Epoch: 25 Average loss: 136.5139\n","====> Test set loss: 137.7669\n","Train Epoch: 26 [0/60000 (0%)]\tLoss: 141.759424\n","Train Epoch: 26 [10000/60000 (17%)]\tLoss: 136.242188\n","Train Epoch: 26 [20000/60000 (33%)]\tLoss: 133.799902\n","Train Epoch: 26 [30000/60000 (50%)]\tLoss: 138.788906\n","Train Epoch: 26 [40000/60000 (67%)]\tLoss: 125.611445\n","Train Epoch: 26 [50000/60000 (83%)]\tLoss: 142.258457\n","====> Epoch: 26 Average loss: 136.4735\n","====> Test set loss: 137.5721\n","Train Epoch: 27 [0/60000 (0%)]\tLoss: 143.938652\n","Train Epoch: 27 [10000/60000 (17%)]\tLoss: 137.534033\n","Train Epoch: 27 [20000/60000 (33%)]\tLoss: 131.970059\n","Train Epoch: 27 [30000/60000 (50%)]\tLoss: 134.011641\n","Train Epoch: 27 [40000/60000 (67%)]\tLoss: 133.366641\n","Train Epoch: 27 [50000/60000 (83%)]\tLoss: 136.233184\n","====> Epoch: 27 Average loss: 136.2459\n","====> Test set loss: 137.4601\n","Train Epoch: 28 [0/60000 (0%)]\tLoss: 136.121934\n","Train Epoch: 28 [10000/60000 (17%)]\tLoss: 147.673623\n","Train Epoch: 28 [20000/60000 (33%)]\tLoss: 139.840654\n","Train Epoch: 28 [30000/60000 (50%)]\tLoss: 133.837041\n","Train Epoch: 28 [40000/60000 (67%)]\tLoss: 130.426680\n","Train Epoch: 28 [50000/60000 (83%)]\tLoss: 131.850273\n","====> Epoch: 28 Average loss: 136.6189\n","====> Test set loss: 137.7939\n","Train Epoch: 29 [0/60000 (0%)]\tLoss: 137.793057\n","Train Epoch: 29 [10000/60000 (17%)]\tLoss: 138.538809\n","Train Epoch: 29 [20000/60000 (33%)]\tLoss: 137.361943\n","Train Epoch: 29 [30000/60000 (50%)]\tLoss: 143.141934\n","Train Epoch: 29 [40000/60000 (67%)]\tLoss: 127.520020\n","Train Epoch: 29 [50000/60000 (83%)]\tLoss: 125.411416\n","====> Epoch: 29 Average loss: 136.5882\n","====> Test set loss: 137.2605\n","Train Epoch: 30 [0/60000 (0%)]\tLoss: 138.948867\n","Train Epoch: 30 [10000/60000 (17%)]\tLoss: 140.738135\n","Train Epoch: 30 [20000/60000 (33%)]\tLoss: 136.909990\n","Train Epoch: 30 [30000/60000 (50%)]\tLoss: 142.461768\n","Train Epoch: 30 [40000/60000 (67%)]\tLoss: 138.425977\n","Train Epoch: 30 [50000/60000 (83%)]\tLoss: 136.799541\n","====> Epoch: 30 Average loss: 136.5484\n","====> Test set loss: 137.4810\n","Train Epoch: 31 [0/60000 (0%)]\tLoss: 140.034033\n","Train Epoch: 31 [10000/60000 (17%)]\tLoss: 142.141025\n","Train Epoch: 31 [20000/60000 (33%)]\tLoss: 132.454277\n","Train Epoch: 31 [30000/60000 (50%)]\tLoss: 137.745674\n","Train Epoch: 31 [40000/60000 (67%)]\tLoss: 135.697012\n","Train Epoch: 31 [50000/60000 (83%)]\tLoss: 133.136963\n","====> Epoch: 31 Average loss: 135.9925\n","====> Test set loss: 137.2122\n","Train Epoch: 32 [0/60000 (0%)]\tLoss: 139.511436\n","Train Epoch: 32 [10000/60000 (17%)]\tLoss: 142.655947\n","Train Epoch: 32 [20000/60000 (33%)]\tLoss: 144.906211\n","Train Epoch: 32 [30000/60000 (50%)]\tLoss: 144.229434\n","Train Epoch: 32 [40000/60000 (67%)]\tLoss: 138.856299\n","Train Epoch: 32 [50000/60000 (83%)]\tLoss: 139.998848\n","====> Epoch: 32 Average loss: 136.2580\n","====> Test set loss: 137.0737\n","Train Epoch: 33 [0/60000 (0%)]\tLoss: 138.813477\n","Train Epoch: 33 [10000/60000 (17%)]\tLoss: 130.679102\n","Train Epoch: 33 [20000/60000 (33%)]\tLoss: 134.376035\n","Train Epoch: 33 [30000/60000 (50%)]\tLoss: 139.061533\n","Train Epoch: 33 [40000/60000 (67%)]\tLoss: 142.095937\n","Train Epoch: 33 [50000/60000 (83%)]\tLoss: 137.466963\n","====> Epoch: 33 Average loss: 136.4302\n","====> Test set loss: 137.3329\n","Train Epoch: 34 [0/60000 (0%)]\tLoss: 137.313662\n","Train Epoch: 34 [10000/60000 (17%)]\tLoss: 133.071045\n","Train Epoch: 34 [20000/60000 (33%)]\tLoss: 133.109414\n","Train Epoch: 34 [30000/60000 (50%)]\tLoss: 138.917871\n","Train Epoch: 34 [40000/60000 (67%)]\tLoss: 133.119561\n","Train Epoch: 34 [50000/60000 (83%)]\tLoss: 128.577490\n","====> Epoch: 34 Average loss: 135.9206\n","====> Test set loss: 136.8950\n","Train Epoch: 35 [0/60000 (0%)]\tLoss: 135.660234\n","Train Epoch: 35 [10000/60000 (17%)]\tLoss: 138.300234\n","Train Epoch: 35 [20000/60000 (33%)]\tLoss: 137.203047\n","Train Epoch: 35 [30000/60000 (50%)]\tLoss: 133.309307\n","Train Epoch: 35 [40000/60000 (67%)]\tLoss: 140.067100\n","Train Epoch: 35 [50000/60000 (83%)]\tLoss: 131.505898\n","====> Epoch: 35 Average loss: 136.0281\n","====> Test set loss: 137.3811\n","Train Epoch: 36 [0/60000 (0%)]\tLoss: 133.166270\n","Train Epoch: 36 [10000/60000 (17%)]\tLoss: 129.822031\n","Train Epoch: 36 [20000/60000 (33%)]\tLoss: 135.072754\n","Train Epoch: 36 [30000/60000 (50%)]\tLoss: 137.551270\n","Train Epoch: 36 [40000/60000 (67%)]\tLoss: 136.213926\n","Train Epoch: 36 [50000/60000 (83%)]\tLoss: 129.728770\n","====> Epoch: 36 Average loss: 135.8181\n","====> Test set loss: 136.7188\n","Train Epoch: 37 [0/60000 (0%)]\tLoss: 130.150850\n","Train Epoch: 37 [10000/60000 (17%)]\tLoss: 127.530410\n","Train Epoch: 37 [20000/60000 (33%)]\tLoss: 138.491006\n","Train Epoch: 37 [30000/60000 (50%)]\tLoss: 130.455908\n","Train Epoch: 37 [40000/60000 (67%)]\tLoss: 143.100225\n","Train Epoch: 37 [50000/60000 (83%)]\tLoss: 135.151953\n","====> Epoch: 37 Average loss: 135.3355\n","====> Test set loss: 137.2955\n","Train Epoch: 38 [0/60000 (0%)]\tLoss: 142.163604\n","Train Epoch: 38 [10000/60000 (17%)]\tLoss: 136.916777\n","Train Epoch: 38 [20000/60000 (33%)]\tLoss: 142.107539\n","Train Epoch: 38 [30000/60000 (50%)]\tLoss: 138.239512\n","Train Epoch: 38 [40000/60000 (67%)]\tLoss: 128.297100\n","Train Epoch: 38 [50000/60000 (83%)]\tLoss: 137.803818\n","====> Epoch: 38 Average loss: 135.3665\n","====> Test set loss: 136.8422\n","Train Epoch: 39 [0/60000 (0%)]\tLoss: 129.663340\n","Train Epoch: 39 [10000/60000 (17%)]\tLoss: 136.734893\n","Train Epoch: 39 [20000/60000 (33%)]\tLoss: 137.086484\n","Train Epoch: 39 [30000/60000 (50%)]\tLoss: 140.688311\n","Train Epoch: 39 [40000/60000 (67%)]\tLoss: 132.242910\n","Train Epoch: 39 [50000/60000 (83%)]\tLoss: 132.605371\n","====> Epoch: 39 Average loss: 135.2071\n","====> Test set loss: 136.6865\n","Train Epoch: 40 [0/60000 (0%)]\tLoss: 134.023838\n","Train Epoch: 40 [10000/60000 (17%)]\tLoss: 133.969619\n","Train Epoch: 40 [20000/60000 (33%)]\tLoss: 133.913633\n","Train Epoch: 40 [30000/60000 (50%)]\tLoss: 135.878193\n","Train Epoch: 40 [40000/60000 (67%)]\tLoss: 136.443213\n","Train Epoch: 40 [50000/60000 (83%)]\tLoss: 142.580166\n","====> Epoch: 40 Average loss: 135.4000\n","====> Test set loss: 136.6593\n","Train Epoch: 41 [0/60000 (0%)]\tLoss: 135.571484\n","Train Epoch: 41 [10000/60000 (17%)]\tLoss: 133.436064\n","Train Epoch: 41 [20000/60000 (33%)]\tLoss: 128.877207\n","Train Epoch: 41 [30000/60000 (50%)]\tLoss: 132.872598\n","Train Epoch: 41 [40000/60000 (67%)]\tLoss: 130.460449\n","Train Epoch: 41 [50000/60000 (83%)]\tLoss: 138.798477\n","====> Epoch: 41 Average loss: 135.1848\n","====> Test set loss: 136.9192\n","Train Epoch: 42 [0/60000 (0%)]\tLoss: 137.495996\n","Train Epoch: 42 [10000/60000 (17%)]\tLoss: 136.939844\n","Train Epoch: 42 [20000/60000 (33%)]\tLoss: 128.582051\n","Train Epoch: 42 [30000/60000 (50%)]\tLoss: 139.499648\n","Train Epoch: 42 [40000/60000 (67%)]\tLoss: 140.257891\n","Train Epoch: 42 [50000/60000 (83%)]\tLoss: 138.920146\n","====> Epoch: 42 Average loss: 135.0725\n","====> Test set loss: 136.5796\n","Train Epoch: 43 [0/60000 (0%)]\tLoss: 130.504238\n","Train Epoch: 43 [10000/60000 (17%)]\tLoss: 130.715479\n","Train Epoch: 43 [20000/60000 (33%)]\tLoss: 134.941533\n","Train Epoch: 43 [30000/60000 (50%)]\tLoss: 127.087988\n","Train Epoch: 43 [40000/60000 (67%)]\tLoss: 138.305215\n","Train Epoch: 43 [50000/60000 (83%)]\tLoss: 126.582959\n","====> Epoch: 43 Average loss: 134.7099\n","====> Test set loss: 136.1958\n","Train Epoch: 44 [0/60000 (0%)]\tLoss: 129.942324\n","Train Epoch: 44 [10000/60000 (17%)]\tLoss: 135.826289\n","Train Epoch: 44 [20000/60000 (33%)]\tLoss: 133.864883\n","Train Epoch: 44 [30000/60000 (50%)]\tLoss: 139.731758\n","Train Epoch: 44 [40000/60000 (67%)]\tLoss: 127.453203\n","Train Epoch: 44 [50000/60000 (83%)]\tLoss: 123.492822\n","====> Epoch: 44 Average loss: 134.9373\n","====> Test set loss: 136.5833\n","Train Epoch: 45 [0/60000 (0%)]\tLoss: 137.395684\n","Train Epoch: 45 [10000/60000 (17%)]\tLoss: 124.475098\n","Train Epoch: 45 [20000/60000 (33%)]\tLoss: 137.442334\n","Train Epoch: 45 [30000/60000 (50%)]\tLoss: 137.531445\n","Train Epoch: 45 [40000/60000 (67%)]\tLoss: 136.416299\n","Train Epoch: 45 [50000/60000 (83%)]\tLoss: 135.773086\n","====> Epoch: 45 Average loss: 134.7338\n","====> Test set loss: 136.0356\n","Train Epoch: 46 [0/60000 (0%)]\tLoss: 135.142490\n","Train Epoch: 46 [10000/60000 (17%)]\tLoss: 143.282783\n","Train Epoch: 46 [20000/60000 (33%)]\tLoss: 131.711123\n","Train Epoch: 46 [30000/60000 (50%)]\tLoss: 140.466240\n","Train Epoch: 46 [40000/60000 (67%)]\tLoss: 130.486855\n","Train Epoch: 46 [50000/60000 (83%)]\tLoss: 126.643828\n","====> Epoch: 46 Average loss: 134.5964\n","====> Test set loss: 136.8388\n","Train Epoch: 47 [0/60000 (0%)]\tLoss: 138.876152\n","Train Epoch: 47 [10000/60000 (17%)]\tLoss: 133.093232\n","Train Epoch: 47 [20000/60000 (33%)]\tLoss: 139.082051\n","Train Epoch: 47 [30000/60000 (50%)]\tLoss: 140.958340\n","Train Epoch: 47 [40000/60000 (67%)]\tLoss: 135.354424\n","Train Epoch: 47 [50000/60000 (83%)]\tLoss: 135.289795\n","====> Epoch: 47 Average loss: 135.2378\n","====> Test set loss: 136.4585\n","Train Epoch: 48 [0/60000 (0%)]\tLoss: 144.884941\n","Train Epoch: 48 [10000/60000 (17%)]\tLoss: 132.901465\n","Train Epoch: 48 [20000/60000 (33%)]\tLoss: 134.095342\n","Train Epoch: 48 [30000/60000 (50%)]\tLoss: 131.852217\n","Train Epoch: 48 [40000/60000 (67%)]\tLoss: 133.711797\n","Train Epoch: 48 [50000/60000 (83%)]\tLoss: 132.322080\n","====> Epoch: 48 Average loss: 134.3256\n","====> Test set loss: 136.2604\n","Train Epoch: 49 [0/60000 (0%)]\tLoss: 137.620635\n","Train Epoch: 49 [10000/60000 (17%)]\tLoss: 126.872812\n","Train Epoch: 49 [20000/60000 (33%)]\tLoss: 134.195508\n","Train Epoch: 49 [30000/60000 (50%)]\tLoss: 133.198496\n","Train Epoch: 49 [40000/60000 (67%)]\tLoss: 135.018691\n","Train Epoch: 49 [50000/60000 (83%)]\tLoss: 131.933408\n","====> Epoch: 49 Average loss: 134.3305\n","====> Test set loss: 136.1129\n","Train Epoch: 50 [0/60000 (0%)]\tLoss: 135.984580\n","Train Epoch: 50 [10000/60000 (17%)]\tLoss: 136.126377\n","Train Epoch: 50 [20000/60000 (33%)]\tLoss: 140.737070\n","Train Epoch: 50 [30000/60000 (50%)]\tLoss: 141.407559\n","Train Epoch: 50 [40000/60000 (67%)]\tLoss: 127.821699\n","Train Epoch: 50 [50000/60000 (83%)]\tLoss: 132.046104\n","====> Epoch: 50 Average loss: 134.8856\n","====> Test set loss: 137.2140\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"24AtAXP7yGHv","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}